# Network-Architecture explored from scratch using Python and NumPy
This repo contains various notebooks dedicated to implementing various neural network architectures from scratch in order to better understand the working of Neural Networks before using frameworks like Tensorflow or PyTorch.

In the first notebook, I explore the workings of a shallow neural network architecture particularly a single hidden layer neural network.
The notebook offers a deep mathematical dive into the basics of forward propagation and back propagation thereby demonstrating how a general neural network would work.
Finally the notebook translates this "mathematical intuition" into code from scratch relying only on NumPy functionalities to develop the Neural Network model.

The second notebook explores the deep neural network architecture and the effects of various regularization techniques (such as L2 Regularization, Dropout, Minibatch gradient descent, SGD with Momentum and Adam optimization) on the performance of the Neural Network.
